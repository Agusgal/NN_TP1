{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-29T21:18:39.595101Z","iopub.execute_input":"2021-09-29T21:18:39.595414Z","iopub.status.idle":"2021-09-29T21:18:39.605671Z","shell.execute_reply.started":"2021-09-29T21:18:39.595374Z","shell.execute_reply":"2021-09-29T21:18:39.604855Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Trabajo Práctico 1\nSe buscó analizar si existe un sesgo en el dataset de snli donde el contenido mismo del texto puede estar delatando si existe una contradicción con respecto a la hipótesis oculta.\n## Carga de Datos\nSe cargaron los datasets correspondientes para observación, ya divididos en datos de entrenamiento, validación y prueba.","metadata":{}},{"cell_type":"code","source":"# Cargo los datos\ndf_train = pd.read_hdf(\"/kaggle/input/sesgos-en-el-dataset-de-snli/train_data.hdf5\")\ndf_valid = pd.read_hdf(\"/kaggle/input/sesgos-en-el-dataset-de-snli/valid_data.hdf5\")\ndf_test = pd.read_hdf(\"/kaggle/input/sesgos-en-el-dataset-de-snli/test_data.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:18:43.742577Z","iopub.execute_input":"2021-09-29T21:18:43.743402Z","iopub.status.idle":"2021-09-29T21:18:49.687545Z","shell.execute_reply.started":"2021-09-29T21:18:43.743353Z","shell.execute_reply":"2021-09-29T21:18:49.686723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.read_csv(\"/kaggle/input/sesgos-en-el-dataset-de-snli/submission_sample.csv\", index_col=\"pairID\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:18:49.688979Z","iopub.execute_input":"2021-09-29T21:18:49.689217Z","iopub.status.idle":"2021-09-29T21:18:49.714249Z","shell.execute_reply.started":"2021-09-29T21:18:49.689191Z","shell.execute_reply":"2021-09-29T21:18:49.713459Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"text_train = df_train[\"text\"].tolist()\nlabels_train = df_train[\"gold_label\"].tolist()\ntext_val = df_valid[\"text\"].tolist()\nlabels_val = df_valid[\"gold_label\"].tolist()\ntext_test = df_test[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T22:30:05.016877Z","iopub.execute_input":"2021-09-29T22:30:05.017339Z","iopub.status.idle":"2021-09-29T22:30:05.050655Z","shell.execute_reply.started":"2021-09-29T22:30:05.017294Z","shell.execute_reply":"2021-09-29T22:30:05.049624Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#Veamos el balance de clases\nfrom collections import Counter\nCounter(labels_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:18:53.190534Z","iopub.execute_input":"2021-09-29T21:18:53.190835Z","iopub.status.idle":"2021-09-29T21:18:53.213575Z","shell.execute_reply.started":"2021-09-29T21:18:53.190803Z","shell.execute_reply":"2021-09-29T21:18:53.212590Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Pre-procesamiento de Texto\n+ NLTK (Natural Language Toolkit)\n  + Lemmatization: reduce a sus significados (ej, quita conjugación verbal)\n  + Stop Words: quita preposiciones (como palabras muy usuales de relleno?)\n  + Stemming: reduce las palabras a su raíz\n  + Filtrado de no palabras","metadata":{}},{"cell_type":"code","source":"# Paquetes de Natural Language Tool Kit\nimport nltk\n#Tokenización (a partir de este se trabajan las otras combinacionies)\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:18:57.590676Z","iopub.execute_input":"2021-09-29T21:18:57.591317Z","iopub.status.idle":"2021-09-29T21:18:59.420992Z","shell.execute_reply.started":"2021-09-29T21:18:57.591271Z","shell.execute_reply":"2021-09-29T21:18:59.419995Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Función con la cual también decido cómo pre-procesar\ndef text_filter(dataset, do_lemm, do_stop, do_stem, do_alpha):\n    texts_filtrados = list()\n    for idx in range(len(dataset.text)):\n        if idx%100==0:\n            print(\"\\r Procesados: {}\".format(idx),end=\"\")\n        em = dataset.text[idx]\n        tok = word_tokenize(em)\n        if do_lemm == True:\n            lem = [lemmatizer.lemmatize(x,pos='v') for x in tok]\n        else:\n            lem = tok\n        if do_stop == True:\n            stop = [x for x in lem if x not in stopwords.words('english')]\n        else:\n            stop = lem\n        if do_stem == True:\n            stem = [stemmer.stem(x) for x in stop]\n        else:\n            stem = stop\n        if do_alpha == True:\n            alpha = [x for x in stem if x.isalpha()]\n        else:\n            alpha = stem\n        texts_filtrados.append(\" \".join(alpha))\n    return texts_filtrados","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:02.081153Z","iopub.execute_input":"2021-09-29T21:19:02.081766Z","iopub.status.idle":"2021-09-29T21:19:02.091574Z","shell.execute_reply.started":"2021-09-29T21:19:02.081724Z","shell.execute_reply":"2021-09-29T21:19:02.090655Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Vectorizadores\n+ Count Vectorizer\n+ TFIDF Vectorizer","metadata":{}},{"cell_type":"code","source":"#Importo los vectorizadores\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:04.616407Z","iopub.execute_input":"2021-09-29T21:19:04.616667Z","iopub.status.idle":"2021-09-29T21:19:04.621560Z","shell.execute_reply.started":"2021-09-29T21:19:04.616641Z","shell.execute_reply":"2021-09-29T21:19:04.620874Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_max = 0.99 # max_df: int para frecuencia contada, float para proporcional\ndf_min = 0.01 # min_df: idem\nn_range = (1,1) # ngram_range: (1,1) default\ncv_cv = CountVectorizer(max_df = df_max, min_df= df_min, ngram_range = n_range)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:06.924781Z","iopub.execute_input":"2021-09-29T21:19:06.925367Z","iopub.status.idle":"2021-09-29T21:19:06.929534Z","shell.execute_reply.started":"2021-09-29T21:19:06.925334Z","shell.execute_reply":"2021-09-29T21:19:06.928650Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_max = 0.99\ndf_min = 0.01\nn_range = (1,1)\ncv_idf = TfidfVectorizer(max_df = df_max, min_df= df_min, ngram_range = n_range)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:09.846584Z","iopub.execute_input":"2021-09-29T21:19:09.847053Z","iopub.status.idle":"2021-09-29T21:19:09.851187Z","shell.execute_reply.started":"2021-09-29T21:19:09.847022Z","shell.execute_reply":"2021-09-29T21:19:09.850456Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_cvs(text_train, text_valid, cv):\n    cv_train = cv.fit_transform(text_train)\n    cv_valid = cv.transform(text_valid)\n    return cv_train, cv_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:11.966775Z","iopub.execute_input":"2021-09-29T21:19:11.967177Z","iopub.status.idle":"2021-09-29T21:19:11.970985Z","shell.execute_reply.started":"2021-09-29T21:19:11.967150Z","shell.execute_reply":"2021-09-29T21:19:11.970459Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Calisificadores\n+ Multinomial Naive-Bayes\n+ Regresión Logística (MLP)","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:14.851986Z","iopub.execute_input":"2021-09-29T21:19:14.852435Z","iopub.status.idle":"2021-09-29T21:19:14.858737Z","shell.execute_reply.started":"2021-09-29T21:19:14.852405Z","shell.execute_reply":"2021-09-29T21:19:14.857842Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Parámetros del Clasificador\na = 1\nclf_NBMN = MultinomialNB(alpha = a)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:16.396496Z","iopub.execute_input":"2021-09-29T21:19:16.397088Z","iopub.status.idle":"2021-09-29T21:19:16.401441Z","shell.execute_reply.started":"2021-09-29T21:19:16.397045Z","shell.execute_reply":"2021-09-29T21:19:16.400857Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Insertar celdas para MLP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Métricas\n+ Primaria:\n + asdf\n+ Secundarias:\n + Precision\n + Recall\n + F1-score\n + ROC-AUC","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:20.680852Z","iopub.execute_input":"2021-09-29T21:19:20.681365Z","iopub.status.idle":"2021-09-29T21:19:20.684803Z","shell.execute_reply.started":"2021-09-29T21:19:20.681333Z","shell.execute_reply":"2021-09-29T21:19:20.684297Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_scores(clf, X_train, y_train, X_valid, y_valid):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    \n    score_train = clf.score(X_train, y_train)\n    score_valid = clf.score(X_valid, y_valid)\n    return (score_train, score_valid)\n    \ndef get_metrics(clf, X_valid, y_valid):\n    y_pred = clf.predict(X_test)\n    m_conf = metrics.confusion_matrix(y_valid, y_pred)\n    precision = metrics.precision_score(y_valid, y_pred)\n    recall_score = metrics.recall_score(y_valid,y_pred)\n    f1_score = metrics.f1_score(y_valid,y_pred)\n    acc = metrics.accuracy_score(y_valid, y_pred)\n    \n    return score_train, score_valid, m_conf, precision, recall_score, f1_score, acc","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:21.943900Z","iopub.execute_input":"2021-09-29T21:19:21.944495Z","iopub.status.idle":"2021-09-29T21:19:21.950879Z","shell.execute_reply.started":"2021-09-29T21:19:21.944461Z","shell.execute_reply":"2021-09-29T21:19:21.950359Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Función que hace todo lo anterior de una\ndef process_data(clf, cv, df_train, df_valid, do_lemm, do_stop, do_stem, do_alpha):\n    print(\"\\n  Filtrando Textos\")\n    texts_train = text_filter(df_train, do_lemm, do_stop, do_stem, do_alpha)\n    texts_valid = text_filter(df_valid, do_lemm, do_stop, do_stem, do_alpha)\n    \n    labels_train = df_train[\"gold_label\"].tolist()\n    labels_valid = df_valid[\"gold_label\"].tolist()\n    \n    print(\"\\nVectorizando\")\n    cv_train, cv_valid = get_cvs(texts_train, texts_valid, cv)\n    \n    print(\"Obteniendo Puntajes\")\n    score_train, score_valid = get_scores(clf, cv_train, labels_train, cv_valid, labels_valid)\n    return (cv_train, cv_valid, score_train, score_valid)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:43:40.148261Z","iopub.execute_input":"2021-09-29T21:43:40.148918Z","iopub.status.idle":"2021-09-29T21:43:40.158025Z","shell.execute_reply.started":"2021-09-29T21:43:40.148880Z","shell.execute_reply":"2021-09-29T21:43:40.156977Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_raw = process_data(clf_NBMN, cv_cv, df_train, df_valid, False, False, False, False)\ntest_lemm = process_data(clf_NBMN, cv_cv, df_train, df_valid, True, False, False, False)\ntest_stop = process_data(clf_NBMN, cv_cv, df_train, df_valid, False, True, False, False)\ntest_stem = process_data(clf_NBMN, cv_cv, df_train, df_valid, False, False, True, False)\ntest_alfa = process_data(clf_NBMN, cv_cv, df_train, df_valid, False, False, False, True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:43:54.236061Z","iopub.execute_input":"2021-09-29T21:43:54.236425Z","iopub.status.idle":"2021-09-29T22:04:56.340771Z","shell.execute_reply.started":"2021-09-29T21:43:54.236370Z","shell.execute_reply":"2021-09-29T22:04:56.339934Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"Train/Valid:\")\nprint(\"raw:  \",  test_raw[2],\"; \",  test_raw[3])\nprint(\"lemm: \", test_lemm[2],\"; \", test_lemm[3])\nprint(\"stop: \", test_stop[2],\"; \", test_stop[3])\nprint(\"stem: \", test_stem[2],\"; \", test_stem[3])\nprint(\"alfa: \", test_alfa[2],\"; \", test_alfa[3])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T22:08:06.856450Z","iopub.execute_input":"2021-09-29T22:08:06.856724Z","iopub.status.idle":"2021-09-29T22:08:06.865403Z","shell.execute_reply.started":"2021-09-29T22:08:06.856698Z","shell.execute_reply":"2021-09-29T22:08:06.864518Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Comparando los puntajes para cada prueba, parece ser que existe una mejora en train cuando se aplica lematización y stemming, mientras que la eliminación de stop words reduce el puntaje y la eliminación de no palabras no parece tener efectos significantes. Esto puede ser porque en caso de observar si una frase contradice a otra por el contenido de su vocabulario, las palabras consideradas como stop words sí están cargadas de significado para este análisis.\n\nRemover las no palabras probablemente sea más conveniente cuando se analice el rango de ngramas.\n\nPor lo tanto, para los siguientes parámetros, se continuará utilizando los datasets solo con Lematización y Stemming y remoción de no-palabras.","metadata":{}},{"cell_type":"code","source":"# Finalmente se trabaja con solo Lemmatization y Stemming\ntext_train_final = text_filter(df_train, True, False, True, False)\ntext_valid_final = text_filter(df_valid, True, False, True, False)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T22:24:06.762315Z","iopub.execute_input":"2021-09-29T22:24:06.762700Z","iopub.status.idle":"2021-09-29T22:27:41.644770Z","shell.execute_reply.started":"2021-09-29T22:24:06.762662Z","shell.execute_reply":"2021-09-29T22:27:41.643717Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Definido el pre-procesamiento se analizó si se obtienen mejores resultados utilizando el CountVectorizer o TFIDFVectorizer.","metadata":{}},{"cell_type":"code","source":"cv_train, cv_valid = get_cvs(text_train_final, text_valid_final, cv_cv)\ncv_scores = get_scores(clf_NBMN, cv_train, labels_train, cv_valid, labels_val)\ncv_train, cv_valid = get_cvs(text_train_final, text_valid_final, cv_idf)\nidf_scores = get_scores(clf_NBMN, cv_train, labels_train, cv_valid, labels_val)\n\nprint(cv_scores)\nprint(idf_scores)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T22:32:05.901259Z","iopub.execute_input":"2021-09-29T22:32:05.901883Z","iopub.status.idle":"2021-09-29T22:32:22.649099Z","shell.execute_reply.started":"2021-09-29T22:32:05.901845Z","shell.execute_reply":"2021-09-29T22:32:22.648202Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Bajo las mismas condiciones el CountVectorizer parece obtener mejores resultados que el TFIDFVectorizer, por lo tanto se continuará trabajando con el CountVectorizer","metadata":{}},{"cell_type":"code","source":"# Empiezo a manipular los parámetros del Vectorizador\ndf_max = 0.99 # max_df: int para frecuencia contada, float para proporcional\ndf_min = 0.01 # min_df: idem\nn_range = (1,1) # ngram_range: (1,1) default\ncv_cv = CountVectorizer(max_df = df_max, min_df= df_min, ngram_range = n_range)\ncv_train, cv_valid = get_cvs(text_train_final, text_valid_final, cv_cv)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:19:06.924781Z","iopub.execute_input":"2021-09-29T21:19:06.925367Z","iopub.status.idle":"2021-09-29T21:19:06.929534Z","shell.execute_reply.started":"2021-09-29T21:19:06.925334Z","shell.execute_reply":"2021-09-29T21:19:06.928650Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"asñdlfkajsdñf","metadata":{}},{"cell_type":"code","source":"clf.fit(cv_train, labels_train)\nlabels_pred = clf.predict(cv_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Veamos cómo funciona el clasificador para train\nclf.score(cv_train, labels_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Veamos cómo funciona el clasificador para valid\nclf.score(cv_valid, labels_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_test_final = text_filter(df_train, True, False, False, False)\ncv_test = cv.transform(text_test_final)\nget_metrics(clf, cv_test, labels_test)\n# test_labels = clf.predict(cv_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Para armar Submission\nUna vez elegido el grado de pre-procesamiento, el vectorizador y el clasificador, lo aplico sobre el el test.\n+ Preprocesamiento: Lemmatization, Stemming y No-Palabras\n+ Vectorizador:\n+ Clasificador:","metadata":{}},{"cell_type":"code","source":"#Armo el submission.csv\ndf_test = pd.DataFrame(data=test_labels, columns=[\"pred_labels\"],)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.index.names = [\"pairID\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusión/Resumen","metadata":{}},{"cell_type":"code","source":"def cv_filtrado(data, df_max, df_min, n_range):\n    texts_filtrados = text_filter(data, True, True, True, True) # En orden: Lemmatization, Stop Words, Stemming, Non-Words\n    count_vect = CountVectorizer(max_df = df_max, min_df = df_min, ngram_range = n_range)\n    X_data = count_vect.fit_transform(texts_filtrados)\n    X_data.toarray()\n    return X_data   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Carga de Datos","metadata":{}},{"cell_type":"markdown","source":"## Pre-procesamiento","metadata":{}},{"cell_type":"markdown","source":"## Vectorización","metadata":{}},{"cell_type":"markdown","source":"## Clasificador","metadata":{}}]}